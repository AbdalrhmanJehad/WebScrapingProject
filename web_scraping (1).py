# -*- coding: utf-8 -*-
"""Web scraping

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zhaCtDfZJgGhiGiywX8_I74LOGMrZMG4
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

base_url='https://www.careopinion.org.uk/opinions/?page={}'
def scrap_page(url):
  response = requests.get(url)
  response.raise_for_status()
  soup = BeautifulSoup(response.content,'lxml')
  books=soup.find_all("a", class_='font-c-1 tooltip')

  story_title = []
  story_date = []
  story_text = []
  story_related_service = []
  story_summary=[]

  base_domain='https://www.careopinion.org.uk'

  for book in books:
    title = book.get_text(strip=True).replace('"', '')
    story_title.append(title)
    href = book.get('href')
    if href:
      full_url = base_domain + href
      opinion_response = requests.get(full_url)
      opinion_response.raise_for_status()
      opinion_soup = BeautifulSoup(opinion_response.content, 'lxml')
      opinion_tag = opinion_soup.find('blockquote', id='opinion_body')
      text = opinion_tag.get_text(strip=True) if opinion_tag else 'No story text found.'
      story_text.append(text)

      date_tag = opinion_soup.find('time')
      date = date_tag.get_text(strip=True) if date_tag else 'No date found.'
      story_date.append(date)

      service_tag=opinion_soup.find(class_="service_location m-margin-w-1")
      related_service = service_tag.get_text(strip=True) if service_tag else 'No related service found.'
      story_related_service.append(related_service)


      summary_tag=opinion_soup.find('div',class_="inner")
      summary=summary_tag.get_text(separator =' ',strip=True) if summary_tag else 'No summary found.'
      story_summary.append(summary)
      time.sleep(2)

  return story_title, story_date, story_text, story_related_service, story_summary



def scrape_all_pages(base_url,num_pages):
  all_title=[]
  all_date=[]
  all_text=[]
  all_related_service=[]
  all_summary=[]
  for page in range(1, num_pages + 1):
    print(f'Page num = {page}')
    url = base_url.format(page)
    titles, dates, texts, services, summaries = scrap_page(url)
    all_title.extend(titles)
    all_date.extend(dates)
    all_text.extend(texts)
    all_related_service.extend(services)
    all_summary.extend(summaries)
    time.sleep(2)
  return all_title, all_date, all_text, all_related_service, all_summary

titles, dates, texts, related_services, summaries = scrape_all_pages(base_url, 17)

df = pd.DataFrame({
    'Title': titles,'Date': dates,'Story Text': texts,'Related Service': related_services,'Story Summary': summaries
})

df.to_csv('care_opinion_data.csv', index=False, sep=';')
print('finish')